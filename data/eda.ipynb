{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T17:59:03.014609Z",
     "start_time": "2023-06-06T17:59:03.010760Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dedupe\n",
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from unidecode import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T17:59:03.672829Z",
     "start_time": "2023-06-06T17:59:03.663383Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5112/2100933013.py:23: DeprecationWarning: `structlog.threadlocal` is deprecated, please use `structlog.contextvars` instead.\n",
      "  context_class=structlog.threadlocal.wrap_dict(dict),\n"
     ]
    }
   ],
   "source": [
    "# logging setup\n",
    "\n",
    "import logging\n",
    "import structlog\n",
    "\n",
    "def logging_setup():\n",
    "    \"\"\"Setup logging for the project\"\"\"\n",
    "    logging.basicConfig(\n",
    "        format=\"%(message)s\",\n",
    "        level=logging.INFO,\n",
    "        handlers=[logging.StreamHandler()],\n",
    "    )\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            structlog.stdlib.filter_by_level,\n",
    "            structlog.stdlib.add_logger_name,\n",
    "            structlog.stdlib.add_log_level,\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            structlog.processors.JSONRenderer(),\n",
    "        ],\n",
    "        logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "        wrapper_class=structlog.stdlib.BoundLogger,\n",
    "        context_class=structlog.threadlocal.wrap_dict(dict),\n",
    "        cache_logger_on_first_use=True,\n",
    "    )\n",
    "\n",
    "    return structlog.get_logger()\n",
    "\n",
    "logger = logging_setup()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing\n",
    "Considering the data is coming from two different sources, we need to preprocess the data to make sure the data is in the same format. After combining the a__ datasets on the geo_id column and the b__ datasets on the b_entity_id column, we will drop various columns to create two pared down datasets. After which, we will be using the following preprocessing steps:\n",
    "- Remove special characters\n",
    "- Remove extra spaces\n",
    "- Remove extra whitespaces\n",
    "- Remove any newlines\n",
    "- Format US and CA postal codes. (The other countries will be ignored for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T17:59:10.418060Z",
     "start_time": "2023-06-06T17:59:07.957429Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p9/rbwr2ck94mbc8ljw8kl8k2j40000gn/T/ipykernel_6693/976535386.py:22: DtypeWarning: Columns (11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  b_address = pd.read_csv(\"b__address.csv\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Preprocess the data\n",
    " This step is necessary to make sure the data is in the same format and we can compare the data\"\"\"\n",
    "\n",
    "a_company = pd.read_csv(\"a__company.csv\")\n",
    "a_geo = pd.read_csv(\"a__geo.csv\")\n",
    "# merge a_company and a_geo on geo_id to create df_a\n",
    "df_a = pd.merge(a_company, a_geo, on=\"geo_id\")\n",
    "df_a = df_a[\n",
    "    [\"vendor_id\", \"name\", \"address\", \"city\", \"state\", \"zipcode_y\", \"country_x\"]\n",
    "]\n",
    "# rename cols\n",
    "df_a = df_a.rename(\n",
    "    columns={\n",
    "        \"vendor_id\": \"id\",\n",
    "        \"address\": \"street\",\n",
    "        \"zipcode_y\": \"postal\",\n",
    "        \"country_x\": \"country\",\n",
    "    }\n",
    ")\n",
    "\n",
    "b_company = pd.read_csv(\"b__company.csv\")\n",
    "b_address = pd.read_csv(\"b__address.csv\")\n",
    "# merge b_company and b_address on address_id to create df_b\n",
    "df_b = pd.merge(b_company, b_address, on=\"b_entity_id\")\n",
    "df_b = df_b[\n",
    "    [\n",
    "        \"b_entity_id\",\n",
    "        \"entity_name\",\n",
    "        \"location_street1\",\n",
    "        \"location_city\",\n",
    "        \"state_province_x\",\n",
    "        \"zip_postal_code\",\n",
    "        \"iso_country_x\",\n",
    "    ]\n",
    "]\n",
    "# rename cols\n",
    "df_b = df_b.rename(\n",
    "    columns={\n",
    "        \"b_entity_id\": \"id\",\n",
    "        \"entity_name\": \"name\",\n",
    "        \"location_street1\": \"street\",\n",
    "        \"location_city\": \"city\",\n",
    "        \"state_province_x\": \"state\",\n",
    "        \"zip_postal_code\": \"postal\",\n",
    "        \"iso_country_x\": \"country\",\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T17:59:12.375169Z",
     "start_time": "2023-06-06T17:59:11.506376Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write the preprocessed data to csv files in data/output for future use\n",
    "df_a.to_csv(\"output/df_a.csv\", index=False)\n",
    "df_b.to_csv(\"output/df_b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T17:59:14.011445Z",
     "start_time": "2023-06-06T17:59:14.005406Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"The combining of the dataframes above performed minimal preprocessing. The overall data is still messy and needs to be cleaned up. The following functions will be used to clean up the data and called in the main function below.\"\"\"\n",
    "\n",
    "def string_manipulations(column):\n",
    "    \"\"\"Remove special characters, extra spaces, extra whitespaces, newlines, punctuation, and deal with some messy nulls in the data\"\"\"\n",
    "    column = unidecode(column)\n",
    "    column = re.sub(r\"[^\\w\\s]\", \"\", column) # remove special characters\n",
    "    column = re.sub(r\"\\s+\", \" \", column) # remove extra spaces\n",
    "    # remove all spaces between words\n",
    "    column = re.sub(r\"\\s\", \"\", column)\n",
    "    column = column.replace(\"\\n\", \"\") # remove newlines\n",
    "    column = column.strip() # remove whitespaces\n",
    "    column = column.upper() # make everything uppercase\n",
    "    # check if column has Nan or null, create a list of those strings, and replace them with NaN as str\n",
    "    nulls_list = [\"NAN\", \"NULL\", \"NONE\", \"N/A\", \"NA\", \"N A\", \"NOT_DEFINED\", \"\"]\n",
    "    # if column is equal to any of the strings in nulls_list, replace with NaN\n",
    "    if column in nulls_list:\n",
    "        column = \"NaN\"\n",
    "\n",
    "    return column\n",
    "\n",
    "def format_postal_code(country, postal):\n",
    "    \"\"\"Format US and CA postal codes\n",
    "    US zip should add a 0 to the front if it's only 4 digits after validation\n",
    "    CA zip should be in the format A1A 1A1\n",
    "    Other countries should return the postal code as is\n",
    "\n",
    "    Args:\n",
    "        country (str): country code\n",
    "        postal (str): postal code\n",
    "\n",
    "    Returns:\n",
    "        str: formatted postal code\n",
    "    \"\"\"\n",
    "    us_zip_regex = re.compile(r\"^\\d{5}(?:[-\\s]\\d{4})?$\")\n",
    "    ca_zip_regex = re.compile(r\"^[A-Za-z]\\d[A-Za-z][ -]?\\d[A-Za-z]\\d$\")\n",
    "    if country == \"US\":\n",
    "        if us_zip_regex.match(postal):\n",
    "            postal = postal.replace(\" \", \"\")\n",
    "            return postal\n",
    "        elif len(postal) == 4:\n",
    "            postal = \"0\" + postal\n",
    "            return postal\n",
    "        elif len(postal) < 4:\n",
    "            return \"NaN\"\n",
    "        else:\n",
    "            return postal\n",
    "    elif country == \"CA\":\n",
    "        if ca_zip_regex.match(postal):\n",
    "            postal = postal.replace(\" \", \"\")\n",
    "            return postal\n",
    "        else:\n",
    "            return postal\n",
    "    else:\n",
    "        return postal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T17:59:15.837424Z",
     "start_time": "2023-06-06T17:59:15.826830Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the preprocessed data\n",
    "\n",
    "def read_and_process(_filename):\n",
    "    data_dict = {}\n",
    "    with open(_filename) as file:\n",
    "        _reader = csv.DictReader(file)\n",
    "        for i, _row in enumerate(_reader):\n",
    "            clean_row = dict([(key, string_manipulations(value)) for (key, value) in _row.items()])\n",
    "            if clean_row[\"country\"] == \"US\" or clean_row[\"country\"] == \"CA\":\n",
    "                clean_row[\"postal\"] = format_postal_code(clean_row[\"country\"], clean_row[\"postal\"])\n",
    "            data_dict[i] = clean_row\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T17:59:24.098258Z",
     "start_time": "2023-06-06T17:59:16.955170Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"Reading and processing data\", \"logger\": \"__main__\", \"level\": \"info\", \"timestamp\": \"2023-06-07T01:53:46.601479Z\"}\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"output\")\n",
    "output_file = output_dir / \"entity_matches_output.csv\"\n",
    "settings_file = output_dir / \"entity_matching_learned_settings\"\n",
    "training_file = output_dir / \"entity_matching_training.csv\"\n",
    "\n",
    "left_file = \"output/df_a.csv\"\n",
    "right_file = \"output/df_b.csv\"\n",
    "\n",
    "logger.info(\"Reading and processing data\")\n",
    "left_data = read_and_process(left_file)\n",
    "right_data = read_and_process(right_file)\n",
    "\n",
    "# save left and right data to csv files as left_data_processed.csv and right_data_processed.csv\n",
    "left_df = pd.DataFrame.from_dict(left_data, orient=\"index\")\n",
    "right_df = pd.DataFrame.from_dict(right_data, orient=\"index\")\n",
    "left_df.to_csv(\"output/left_data_processed.csv\", index=False)\n",
    "right_df.to_csv(\"output/right_data_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-06T18:02:20.293989Z",
     "start_time": "2023-06-06T18:02:19.940471Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000BFGE</td>\n",
       "      <td>LOTSOFFCORP</td>\n",
       "      <td>1201AUSTINHIGHWAY</td>\n",
       "      <td>SANANTONIO</td>\n",
       "      <td>TX</td>\n",
       "      <td>782094859</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000P08E</td>\n",
       "      <td>LASERSCOPEINC</td>\n",
       "      <td>3070ORCHARDDRIVE</td>\n",
       "      <td>SANJOSE</td>\n",
       "      <td>CA</td>\n",
       "      <td>951342011</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000JF6E</td>\n",
       "      <td>LONGWENGROUPCORP</td>\n",
       "      <td>17116PRAIRIESTREET</td>\n",
       "      <td>NORTHRIDGE</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85258</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000JF6E</td>\n",
       "      <td>LONGWENGROUPCORP</td>\n",
       "      <td>3625COVEPOINTDRIVE</td>\n",
       "      <td>SALTLAKECITY</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85258</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000JF6E</td>\n",
       "      <td>LONGWENGROUPCORP</td>\n",
       "      <td>7702EASTDOUBLETREERANCHROAD</td>\n",
       "      <td>SCOTTSDALE</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85258</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id              name                       street          city state  \\\n",
       "0  000BFGE       LOTSOFFCORP            1201AUSTINHIGHWAY    SANANTONIO    TX   \n",
       "1  000P08E     LASERSCOPEINC             3070ORCHARDDRIVE       SANJOSE    CA   \n",
       "2  000JF6E  LONGWENGROUPCORP           17116PRAIRIESTREET    NORTHRIDGE    AZ   \n",
       "3  000JF6E  LONGWENGROUPCORP           3625COVEPOINTDRIVE  SALTLAKECITY    AZ   \n",
       "4  000JF6E  LONGWENGROUPCORP  7702EASTDOUBLETREERANCHROAD    SCOTTSDALE    AZ   \n",
       "\n",
       "      postal country  \n",
       "0  782094859      US  \n",
       "1  951342011      US  \n",
       "2      85258      US  \n",
       "3      85258      US  \n",
       "4      85258      US  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in left_data_processed.csv and right_data_processed.csv\n",
    "left_data = pd.read_csv(\"output/left_data_processed.csv\")\n",
    "right_data = pd.read_csv(\"output/right_data_processed.csv\")\n",
    "\n",
    "left_data.head()\n",
    "right_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"Reading and processing data\", \"logger\": \"__main__\", \"level\": \"info\", \"timestamp\": \"2023-06-06T03:59:58.311728Z\"}\n",
      "{\"event\": \"Creating a labeled data set\", \"logger\": \"__main__\", \"level\": \"info\", \"timestamp\": \"2023-06-06T04:00:01.989662Z\"}\n",
      "{\"event\": \"Record link complete\", \"logger\": \"__main__\", \"level\": \"info\", \"timestamp\": \"2023-06-06T04:00:01.991492Z\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = Path(\"output\")\n",
    "output_file = output_dir / \"entity_matches_output.csv\"\n",
    "settings_file = output_dir / \"entity_matching_learned_settings\"\n",
    "training_file = output_dir / \"entity_matching_training.csv\"\n",
    "\n",
    "left_file = \"output/df_a.csv\"\n",
    "right_file = \"output/df_b.csv\"\n",
    "\n",
    "logger.info(\"Reading and processing data\")\n",
    "left_data = read_and_process(left_file)\n",
    "right_data = read_and_process(right_file)\n",
    "\n",
    "logger.info(\"Creating a labeled data set\")\n",
    "\n",
    "fields = [\n",
    "    {'field': 'id', 'type': 'String'},\n",
    "    {'field': 'name', 'type': 'String'},\n",
    "    {'field': 'postal', 'type': 'String', 'has missing': True},\n",
    "    {'field': 'country', 'type': 'ShortString', 'has missing': True}\n",
    "]\n",
    "\n",
    "linker = dedupe.RecordLink(fields)\n",
    "\n",
    "logger.info(\"Record link complete\")\n",
    "\n",
    "# set sample size to ten percent of the data\n",
    "# sample_size = int(len(left_data) * 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"creating labeled examples from output/entity_matching_training.csv\", \"logger\": \"__main__\", \"level\": \"info\", \"timestamp\": \"2023-06-06T04:00:02.077645Z\"}\n",
      "Removing stop word 04\n",
      "Removing stop word 60\n",
      "Removing stop word 00\n",
      "Removing stop word 10\n",
      "Removing stop word 21\n",
      "Removing stop word 02\n",
      "Removing stop word 40\n",
      "Removing stop word 01\n",
      "Removing stop word 11\n",
      "Removing stop word 12\n",
      "Removing stop word 14\n",
      "Removing stop word 20\n",
      "Removing stop word 50\n",
      "Removing stop word 70\n",
      "Removing stop word 03\n",
      "Removing stop word 80\n",
      "Removing stop word 30\n",
      "Removing stop word 06\n",
      "Removing stop word 0C\n",
      "Removing stop word 09\n",
      "Removing stop word 0D\n",
      "Removing stop word 0F\n",
      "Removing stop word 06\n",
      "Removing stop word 0H\n",
      "Removing stop word 05\n",
      "Removing stop word 07\n",
      "Removing stop word 0G\n",
      "Removing stop word 00\n",
      "Removing stop word 0B\n",
      "Removing stop word  I\n",
      "Removing stop word  T\n",
      "Removing stop word CH\n",
      "Removing stop word EC\n",
      "Removing stop word ES\n",
      "Removing stop word IE\n",
      "Removing stop word IN\n",
      "Removing stop word LA\n",
      "Removing stop word LO\n",
      "Removing stop word NC\n",
      "Removing stop word NT\n",
      "Removing stop word OG\n",
      "Removing stop word OL\n",
      "Removing stop word S \n",
      "Removing stop word T \n",
      "Removing stop word TE\n",
      "Removing stop word UN\n",
      "Removing stop word  S\n",
      "Removing stop word AR\n",
      "Removing stop word N \n",
      "Removing stop word ON\n",
      "Removing stop word OR\n",
      "Removing stop word RS\n",
      "Removing stop word SA\n",
      "Removing stop word ST\n",
      "Removing stop word TO\n",
      "Removing stop word VE\n",
      "Removing stop word  O\n",
      "Removing stop word  P\n",
      "Removing stop word AG\n",
      "Removing stop word NG\n",
      "Removing stop word RT\n",
      "Removing stop word  R\n",
      "Removing stop word AL\n",
      "Removing stop word AN\n",
      "Removing stop word EA\n",
      "Removing stop word LE\n",
      "Removing stop word LT\n",
      "Removing stop word RE\n",
      "Removing stop word TY\n",
      "Removing stop word  C\n",
      "Removing stop word  L\n",
      "Removing stop word CA\n",
      "Removing stop word CO\n",
      "Removing stop word E \n",
      "Removing stop word IO\n",
      "Removing stop word LU\n",
      "Removing stop word NS\n",
      "Removing stop word P \n",
      "Removing stop word SO\n",
      "Removing stop word TD\n",
      "Removing stop word TI\n",
      "Removing stop word AD\n",
      "Removing stop word DE\n",
      "Removing stop word ER\n",
      "Removing stop word IT\n",
      "Removing stop word NI\n",
      "Removing stop word OM\n",
      "Removing stop word R \n",
      "Removing stop word Y \n",
      "Removing stop word A \n",
      "Removing stop word RA\n",
      "Removing stop word  B\n",
      "Removing stop word  D\n",
      "Removing stop word DI\n",
      "Removing stop word IS\n",
      "Removing stop word K \n",
      "Removing stop word LI\n",
      "Removing stop word NE\n",
      "Removing stop word RI\n",
      "Removing stop word TR\n",
      "Removing stop word  M\n",
      "Removing stop word AB\n",
      "Removing stop word AC\n",
      "Removing stop word DA\n",
      "Removing stop word EL\n",
      "Removing stop word IC\n",
      "Removing stop word RO\n",
      "Removing stop word EN\n",
      "Removing stop word GE\n",
      "Removing stop word HA\n",
      "Removing stop word LL\n",
      "Removing stop word MB\n",
      "Removing stop word SE\n",
      "Removing stop word UR\n",
      "Removing stop word  E\n",
      "Removing stop word AT\n",
      "Removing stop word D \n",
      "Removing stop word L \n",
      "Removing stop word NA\n",
      "Removing stop word ND\n",
      "Removing stop word PR\n",
      "Removing stop word  G\n",
      "Removing stop word  H\n",
      "Removing stop word G \n",
      "Removing stop word GR\n",
      "Removing stop word IA\n",
      "Removing stop word O \n",
      "Removing stop word OU\n",
      "Removing stop word ME\n",
      "Removing stop word EM\n",
      "Removing stop word ET\n",
      "Removing stop word H \n",
      "Removing stop word  F\n",
      "Removing stop word LC\n",
      "Removing stop word TA\n",
      "Removing stop word  A\n",
      "Removing stop word MA\n",
      "Removing stop word VI\n",
      "Removing stop word AS\n",
      "Removing stop word CE\n",
      "Removing stop word SS\n",
      "Removing stop word BE\n",
      "Removing stop word CI\n",
      "Removing stop word I \n",
      "Removing stop word US\n",
      "Removing stop word PA\n",
      "Removing stop word HE\n",
      "Removing stop word HO\n",
      "Removing stop word IL\n",
      "Removing stop word SI\n",
      "Removing stop word LD\n",
      "Removing stop word INC\n",
      "Removing stop word LTD\n",
      "Removing stop word CO\n",
      "Removing stop word LLC\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mcreating labeled examples from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m training_file)\n\u001b[0;32m----> 7\u001b[0m     linker\u001b[39m.\u001b[39;49mprepare_training(left_data, right_data)\n\u001b[1;32m      9\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mStarting active labeling...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m dedupe\u001b[39m.\u001b[39mconsole_label(linker)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dedupe/api.py:1492\u001b[0m, in \u001b[0;36mLink.prepare_training\u001b[0;34m(self, data_1, data_2, training_file, sample_size, blocked_proportion)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[39m# We need the active learner to know about all our\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m \u001b[39m# existing training data, so add them to data dictionaries\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m examples, y \u001b[39m=\u001b[39m flatten_training(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_pairs)\n\u001b[0;32m-> 1492\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_learner \u001b[39m=\u001b[39m labeler\u001b[39m.\u001b[39;49mRecordLinkDisagreementLearner(\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_model\u001b[39m.\u001b[39;49mpredicates,\n\u001b[1;32m   1494\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_model\u001b[39m.\u001b[39;49mdistances,\n\u001b[1;32m   1495\u001b[0m     data_1,\n\u001b[1;32m   1496\u001b[0m     data_2,\n\u001b[1;32m   1497\u001b[0m     index_include\u001b[39m=\u001b[39;49mexamples,\n\u001b[1;32m   1498\u001b[0m )\n\u001b[1;32m   1500\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_learner\u001b[39m.\u001b[39mmark(examples, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dedupe/labeler.py:457\u001b[0m, in \u001b[0;36mRecordLinkDisagreementLearner.__init__\u001b[0;34m(self, candidate_predicates, featurizer, data_1, data_2, index_include)\u001b[0m\n\u001b[1;32m    454\u001b[0m index_include \u001b[39m=\u001b[39m index_include\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    455\u001b[0m index_include\u001b[39m.\u001b[39mappend(exact_match)\n\u001b[0;32m--> 457\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocker \u001b[39m=\u001b[39m RecordLinkBlockLearner(\n\u001b[1;32m    458\u001b[0m     candidate_predicates, data_1, data_2, index_include\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocker\u001b[39m.\u001b[39mcandidates\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatcher \u001b[39m=\u001b[39m MatchLearner(featurizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcandidates)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dedupe/labeler.py:296\u001b[0m, in \u001b[0;36mRecordLinkBlockLearner.__init__\u001b[0;34m(self, candidate_predicates, data_1, data_2, index_include)\u001b[0m\n\u001b[1;32m    293\u001b[0m sampled_records_2 \u001b[39m=\u001b[39m sample_records(index_data, N_SAMPLED_RECORDS)\n\u001b[1;32m    295\u001b[0m preds \u001b[39m=\u001b[39m _filter_canopy_predicates(candidate_predicates, canopies\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_learner \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39;49mRecordLinkBlockLearner(\n\u001b[1;32m    297\u001b[0m     preds, sampled_records_1, sampled_records_2, index_data\n\u001b[1;32m    298\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample(\n\u001b[1;32m    301\u001b[0m     sampled_records_1, sampled_records_2, N_SAMPLED_RECORD_PAIRS\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    304\u001b[0m examples_to_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcandidates\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dedupe/training.py:278\u001b[0m, in \u001b[0;36mRecordLinkBlockLearner.__init__\u001b[0;34m(self, predicates, sampled_records_1, sampled_records_2, data_2)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocker \u001b[39m=\u001b[39m blocking\u001b[39m.\u001b[39mFingerprinter(predicates)\n\u001b[1;32m    276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocker\u001b[39m.\u001b[39mindex_all(data_2)\n\u001b[0;32m--> 278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomparison_cover \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoveredPairs(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocker, sampled_records_1, sampled_records_2\n\u001b[1;32m    280\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dedupe/training.py:309\u001b[0m, in \u001b[0;36mRecordLinkBlockLearner.coveredPairs\u001b[0;34m(self, blocker, records_1, records_2)\u001b[0m\n\u001b[1;32m    307\u001b[0m current_blocks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(cover[predicate])\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, record \u001b[39min\u001b[39;00m records_1\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 309\u001b[0m     blocks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(predicate(record))\n\u001b[1;32m    310\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m blocks \u001b[39m&\u001b[39m current_blocks:\n\u001b[1;32m    311\u001b[0m         cover[predicate][block][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dedupe/predicates.py:272\u001b[0m, in \u001b[0;36mSearchPredicate.__call__\u001b[0;34m(self, record, target, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     centers \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_doc_to_id[doc]]\n\u001b[1;32m    271\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     centers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49msearch(doc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthreshold)\n\u001b[1;32m    273\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(\u001b[39mstr\u001b[39m(center) \u001b[39mfor\u001b[39;00m center \u001b[39min\u001b[39;00m centers)\n\u001b[1;32m    274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache[(column, target)] \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dedupe/levenshtein.py:30\u001b[0m, in \u001b[0;36mLevenshteinIndex.search\u001b[0;34m(self, doc, threshold)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(\u001b[39mself\u001b[39m, doc: \u001b[39mstr\u001b[39m, threshold: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     matching_docs \u001b[39m=\u001b[39m Levenshtein_search\u001b[39m.\u001b[39;49mlookup(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex_key, doc, threshold)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m matching_docs:\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_doc_to_id[match] \u001b[39mfor\u001b[39;00m match, _, _ \u001b[39min\u001b[39;00m matching_docs]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(training_file):\n",
    "    logger.info('reading labeled examples from %s' % training_file)\n",
    "    with open(training_file) as tf:\n",
    "        linker.prepare_training(left_data, right_data, training_file=tf)\n",
    "else:\n",
    "    logger.info('creating labeled examples from %s' % training_file)\n",
    "    linker.prepare_training(left_data, right_data)\n",
    "\n",
    "logger.info(\"Starting active labeling...\")\n",
    "\n",
    "dedupe.console_label(linker)\n",
    "\n",
    "logger.info(\"finished console labeling\")\n",
    "\n",
    "linker.train()\n",
    "\n",
    "logger.info(\"Training complete. Saving learned settings to %s\" % settings_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(settings_file, 'wb') as sf:\n",
    "    linker.write_settings(sf)\n",
    "logger.info(\"Learned settings saved. Saving training data to %s\" % training_file)\n",
    "\n",
    "\n",
    "with open(training_file, 'w') as tf:\n",
    "    linker.write_training(tf)\n",
    "logger.info(\"Training data saved. Clustering...\")\n",
    "\n",
    "logger.info(\"Clustering...\")\n",
    "linked_records = linker.join(left_data, right_data, threshold=0.0)\n",
    "\n",
    "logger.info(\"# duplicate sets %s\" % len(linked_records))\n",
    "\n",
    "cluster_membership = {}\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    logger.info(\"clustering id %s\" % cluster_id)\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = {\n",
    "            \"cluster id\": cluster_id,\n",
    "            \"confidence\": score\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Writing results to %s\" % output_file)\n",
    "\n",
    "with open(output_file, 'w') as _output_file:\n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate((left_file, right_file)):\n",
    "        with open(filename) as file_input:\n",
    "            reader = csv.DictReader(file_input)\n",
    "\n",
    "            if header_unwritten:\n",
    "\n",
    "                filenames = (['cluster id', 'confidence', 'source file'] +\n",
    "                                reader.fieldnames)\n",
    "\n",
    "                writer = csv.DictWriter(_output_file, filenames)\n",
    "                writer.writeheader()\n",
    "\n",
    "                header_unwritten = False\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                logger.info(\"writing row %s\" % row_id)\n",
    "                record_id = filename + str(row_id)\n",
    "                cluster_details = cluster_membership.get(record_id, {})\n",
    "                row['source file'] = fileno\n",
    "                row.update(cluster_details)\n",
    "\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T01:14:16.165756Z",
     "start_time": "2023-06-07T01:14:15.593206Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26960/2446607882.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  entity_matching_output = pd.read_csv(\"output/entity_matching_output.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                       0\n",
       "name                     0\n",
       "street               35345\n",
       "city                 32808\n",
       "state                41604\n",
       "postal               43579\n",
       "country                 20\n",
       "cluster_id          161226\n",
       "confidence_score    161226\n",
       "source_file              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load entity_matching_output.csv\n",
    "import pandas as pd\n",
    "entity_matching_output = pd.read_csv(\"output/entity_matching_output.csv\")\n",
    "entity_matching_output.head()\n",
    "\n",
    "# show null values\n",
    "entity_matching_output.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
